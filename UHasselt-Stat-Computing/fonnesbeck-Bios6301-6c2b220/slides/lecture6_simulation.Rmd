---
title: "Simulation"
output: pdf_document
---

## Stochastic Simulation

We often want to obtain a simulated result using a model system of interest.

Using artificially-generated *random numbers*, we can generate realizations of data that would be expected under a particular model.

Input: $\mathbf{X}_i = (X_{1i}, \ldots, X_{ki})$

Output: $\mathbf{Y}_i = h(\mathbf{X}_i)$

Analysis:

$$Pr(h(\mathbf{X}) \le b) \approx \frac{1}{N} \sum_{i=1}^N I(h(\mathbf{X}\_i) \le b)$$
$$E(h(\mathbf{X})) \approx \frac{1}{N} \sum_{i=1}^N h(\mathbf{X}_i)$$

---

## Random Number Seed

The sequence of pseudo-random numbers depends on the initial condition,
or seed.

In R, it is stored in `.Random.seed`, a global variable. To reproduce
results exactly, set the seed:

```{r}
runif(2)
old.seed <- .Random.seed # Store the seed
set.seed(20090425)
runif(2)
set.seed(20090425)  # Reset it
runif(2)
.Random.seed <- old.seed  # Restore old seed
```

`runif()` generates uniform random variables according to the selected PRNG:

```{r}
RNGkind()
RNGkind('Wich')
RNGkind()
```

---

## Generating Discrete Random Variables

If we know the cumulative mass function (CMF), we can generate pseudorandom draws using $U(0,1)$ realizations.

Given CMF *F*:

1. Initialize $x_0 = 0$
2. Generate $u \sim U(0,1)$
3. While $F(x_i) < u$, increment $x_{i+1} = x_i + 1$

Stop for first value of $x_i$ that has cumulative probability greater than sampled uniform.

---

## Example: Binomial

The binomial distribution:

$$f(x|p) = {n \choose x} p^x (1-p)^{n-x}$$

We calculate the CMF of the binomial distribution by summing over the PMF:

```{r}
binom_cdf <- function(x, n, p) {
    Fx <- 0
    for (i in 0:x) {
        Fx <- Fx + choose(n, i) * p^i * (1-p)^(n-i)
    }
    return(Fx)
}

# same thing, different way
binom_pmf <- function(x, n, p) choose(n, x) * p^x * (1-p)^(n-x)
binom_cdf <- function(x, n, p) sum(sapply(seq(0,x), binom_pmf, n, p))
```

Now, we can use this code to generate binomial variates from a uniform draw:

```{r}
gen_variate <- function(F, ...) {
    x <- 0
    u <- runif(1)
    while (F(x, ...) < u) x <- x + 1
    return(x)
}
```

---

## Example: Binomial

```{r}
x <- numeric(100)
for (i in 1:100) x[i] <- gen_variate(binom_cdf, 10, 0.25)
# use replicate function
x <- replicate(100, gen_variate(binom_cdf, 10, 0.25))
hist(x, prob=T)
lines(0:10, dbinom(0:10,10,0.25))
```

---

## Built-in Random Number Generators

R includes random number generators for the most common distributions: `runif`, `rnorm`, `rbinom`, `rpois`, `rexp`, etc.  See ?Distributions

First argument is always `n`, number of variates to generate.

Subsequent arguments are parameters corresponding to the distribution:

```{r}
rnorm(n=5, mean=3, sd=4)
rbinom(n=5, 10, 0.3)
```

---

## Example: Power

Write a simulation to obtain the power from a one-sample t-test.

```{r}
getpower = function(delta, delta.sd, siglevel, n, nsim) {
  pvals = numeric(nsim)
  set.seed(1)
  for(i in seq_along(pvals)) {
    x = rnorm(n, mean=delta, sd=delta.sd)
    pvals[i] = t.test(x, alternative="two.sided", mu=0)$p.value 
  }
  mean(pvals < siglevel)
}
getpower(delta=0.5, delta.sd=2, siglevel=0.05, n=25, nsim=1000)
getpower(delta=0.5, delta.sd=2, siglevel=0.05, n=25, nsim=10000)
# note there is a function to do this
power.t.test(n=25, delta=0.5, sd=2, sig.level=0.05, type='one.sample')$power
```

---

## Exercise

How could you use `getpower` to determine the sample size needed to have a power of 90%?

```{r}
power.t.test(power=0.9, delta=0.5, sd=2, sig.level=0.05, type='one.sample')$n
```

---

## Bootstrapping

Bootstrapping (Efron 1977) is a prominent simulation method in modern statistics. It is a *resampling* method for deriving a sampling distribution for a statistic. It can be used to obtain:

* standard errors
* percentile points
* proportions
* odds ratios
* correlation coefficients

Bootstrapping uses sample data as a population from which new samples are drawn.

![bootstrap](images/bootstrap.png)

---

## Classical Inference

The sample $S$ is a simple or independent sample from $P$:

$$ P = \{x_1, x_2, \ldots, x_N\}$$
$$\text{(population)}$$

$$ S = \{x_1, x_2, \ldots, x_n\}$$
$$\text{(sample)}$$

We wish to make some inference regarding a population parameter, based on a statistical estimate:

$$ \theta = h(P)$$
$$\text{(population parameter)}$$

$$ T = h(S)$$
$$\text{(estimate)}$$

## Presenter Notes

make assumptions about pop. structure

use assumptions to derive sampling distribution for T

---

## Problems

Classical inference can be **non-robust**:

* inaccurate if parametric assumptions are violated
* if we rely on asymptotic results, we may not achieve an acceptable level of accuracy

Classical inference can be **difficult**:

* derivation of sampling distribution may not be possible

An alternative is to estimate the sampling distribution of a statistic *empirically* without making assumptions about the form of the population.

---

## Non-parametric Bootstrap

Bootstrap sample:

$$S_1^* = \{x_{11}^*, x_{12}^*, \ldots, x_{1n}^*\}$$

$S_i^*$ is a sample of size $n$, with replacement.

In R, the function `sample` draws draw random sample of size points from x, optionally with replacement and/or weights:

    !r
    sample(x, size, replace=FALSE, prob=NULL)

`x` can be anything with a `length`; sample(x) does a random permutation.

```{r}
sample(c(-1,14,3,6,-3))
sample(5)
sample(5, replace=T)
```

---

## Bootstrap sample

In R:

```{r}
(x <- rnorm(10))
sample(x, 10, replace=TRUE)
```

## Presenter Notes

We regard S as an "estimate" of population P

population : sample :: sample : bootstrap sample

---

## Non-parametric Bootstrap

Generate replicate bootstrap samples:

$$S^* = \{S_1^*, S_2^*, \ldots, S_R^*\}$$

Compute statistic (estimate) for each bootstrap sample:

$$T_i^* = t(S^*)$$

---

## Example

```{r}
x <- rnorm(10)
s <- numeric(1000)
for(i in seq_along(s)) s[i] <- mean(sample(x, 10, replace=TRUE))
hist(s, xlab="Bootstrap means", main="")
```

---

## Exercise

Re-formulate the previous example using the `replicate` function, rather than a `for` loop.

---

## Bootstrap Estimates

From our bootstrapped samples, we can extract *estimates* of the expectation and variance:

$$\bar{T}^* = \hat{E}(T^*) = \frac{\sum_i T_i^*}{R}$$

$$\hat{\text{Var}}(T^*) = \frac{\sum_i (T_i^* - \bar{T}^*)^2}{R-1}$$

Since we have estimated the expectation of the bootstrapped statistics, we can estimate the **bias** of T:

$$\hat{B}^* = \bar{T}^* - T$$

## Presenter Notes

estimate of T - theta

---

## Error

There are two sources of error in bootstrap estimates:

1. **Sampling error** from the selection of $S$.
2. **Bootstrap error** from failing to enumerate all possible bootstrap samples.

---

## Bootstrap Confidence Intervals

We can use the bootstrap estimates of sampling variance and bias, and by applying normal theory, estimate confidence intervals for statistic $T$:

$$ (T-\hat{B}^*) \pm z_{1-\alpha/2} \sqrt{\widehat{\text{Var}}(T^*)}$$

---

## Bootstrap Percentile Intervals

An alternative approach is to use the empirical quantiles of the bootstrapped statistics. This employs the *ordered* bootstrap replicates:

$$T_{(1)}^*, T_{(2)}^*, \ldots, T_{(R)}^*$$

Simply extract the $100(\alpha/2)$ and $100(1-\alpha/2)$ percentiles:

$$T_{[(R+1)\alpha/2]}^* < \theta < T_{[(R+1)(1-\alpha/2)]}^*$$

## Presenter Notes

Square brackets indicate rounding to nearest integer.

---

## Package `boot`

`boot` provides functions for bootstrapping and related resampling methods.

```{r}
library(boot)
x <- rnorm(10)
med <- function(x,i) median(x[i])
(bmed <- boot(x, med, R=999))
boot.ci(bmed)
```

---

## Example: Anorexia data

Normal distribution?

```{r}
data(anorexia, package="MASS")
hist(anorexia[,'Prewt'], breaks=ceiling(diff(range(anorexia[,'Prewt']))), main='', xlab='Weight')
```

## Presenter Notes

Pre-treatment weights for anorexia patients

---

## Example: Anorexia data

Let's calculate a bootstrapped confidence interval for pre-treatment weights:

```{r}
weight <- anorexia[,'Prewt']
bmeans <- replicate(999, mean(sample(weight, replace=TRUE)))
quantile(bmeans, c(0.025, 0.975))
```

Compare to parametric CI, and `boot`:

```{r}
mean(weight, na.rm=T) + c(-1.96, 1.96)*sd(weight, na.rm=TRUE)/sqrt(length(weight))
weight_boot <- boot(weight, function(x,i) mean(x[i]), R=999)
quantile(weight_boot$t, c(0.025, 0.975))
```

---

## Estimating Coverage Probability

*Coverage probability* is the proportion of time that a calculated confidence interval contains the true parameter value.

* we hope it is the same as the nominal probability!

Several ways of calculating confidence intervals:

* Bootstrap Intervals
* Theoretical Intervals
* Asymptotic Intervals

All confidence intervals are interpreted in light of assumptions:

* data are generated via a model
* $n \rightarrow \infty$

What happens when assumptions are violated? Robust?

---

## Estimating Coverage Probability

We can use simulation to assess an interval's coverage probability.

* Repeat:
    1. Sample data from a statistical model, with known parameters
    2. Calculate CI for simulated data
    3. Test whether calculated interval includes parameter
* Calculate proportion of times interval contained parameter

---

## R code

Calculating the coverage probability for normal bootstrap interval:

```{r}
true_mu <- 0
x <- rnorm(100, true_mu)
R <- 999

lower <- numeric(R)
upper <- numeric(R)

for (i in 1:R) {

    s <- x[sample(length(x), replace=TRUE)]
    xbar <- mean(s)
    s <- sd(s)

    lower[i] = xbar + qnorm(0.025) * (s / sqrt(length(x)))
    upper[i] = xbar + qnorm(0.975) * (s / sqrt(length(x)))
}

mean(lower < true_mu & upper > true_mu)
```

---

Additional Reading
------------------------------

---

## Random Numbers

What is a ***random*** number?

How do we get random numbers?

Computers are intrinsically deterministic, so how are random numbers generated deterministically?

## Presenter Notes

No pattern?
Unpredictable?
True random numbers require actual unpredictability arising from genuinely physical processes such as radioactive decay, photon emissions or atmospheric noise

---

## Pseudo-random Numbers

Computers cannot generate random numbers, only *pseudo*-random numbers.

* can be seeded at an arbitrary starting state and produce the same sequence each time.

We need a pseudo-random number generator (PRNG)

---

## Middle Square Method

An early algorithm by Von Neumann

1. specify a 10-digit number:

```{r}
x <- 5492364201
```

2. square the number (usually becomes a 20-digit number)

```{r}
(y <- as.character(x**2))
```

3. take middle 10 digits as a random number

```{r}
options(scipen=10)
as.numeric(substring(y,6,15))
```

4. repeat as necessary

## Presenter Notes

The generated numbers are not random, but hard to predict.
not a good PRNG, due to short cycles.

---

## Mersenne Twister Algorithm

Relatively new algorithm (1997)

* linear congruential generator
* period = $2^{19937}-1$

Algorithm:

* initialize $X_0 \in \{0,1,\ldots,m-1\}$
* choose "big" numbers $A,B$
* generate sequence of numbers via:

$$X_{i+1} = (AX_i) + B \mod m$$

* divide numbers by *m* to get $X_i \in [0,1)$

For well-chosen A,B the sequence is indistinguishable from $U(0,1)$

---

## Example: Mersenne-Twister

```{r}
m <- 10; A <- 103; B <- 17; x <- 2
(x <- (A*x + B) %% m)
(x <- (A*x + B) %% m)
(x <- (A*x + B) %% m)
```

`m=10` so maximum period is 10.

Better:

```{r}
m <- 2^32; A <- 1644525; B <- 1013904223
x <- (A*x + B) %% m; x/m
x <- (A*x + B) %% m; x/m
x <- (A*x + B) %% m; x/m
```

---

## Generating True Random Numbers

![random.org](http://d.pr/i/42tP+)

## Presenter Notes

Gathers atmospheric noise via a radio-receiver card tuned to an unused frequency and connected to a computer where it is sampled and digitized.

---

## Package `random`

Accesses the true random number service at http://random.org.

* `randomNumber` retrieves random integers with duplicates
* `randomSequence` retrieves random sequences without duplicates
* `randomStrings` retrieves strings

Requires an internet connection!

    !r
    > library(random)
    > randomNumbers(10, 1, 100)
         V1 V2 V3 V4 V5
    [1,] 65 47 58 19 39
    [2,] 67 50 14 55 62
    > randomStrings(5, 10)
         V1
    [1,] "45uvtdAEjp"
    [2,] "MNsAXi0UYD"
    [3,] "bBF8oin409"
    [4,] "QSJJCnp2Uo"
    [5,] "ltWlBUFTKQ"

---

## Exercise

Can you come up with a simple, vectorized function for generating samples from a binomial distribution?

*Hint*: take advantage of the fact that a binomial variable is the sum of *n* Bernoulli random variables.

## Presenter Notes

sum(runif(n) < p)

---

## Example: Geometric random variables

The geometric distribution models the number of "failure" events expected before a "success" event occurs.

* e.g. the number of heads expected from a fair coin before tails appears.

```{r}
geom_sample <- function(p) {
    x <- 0
    success <- FALSE
    while (!success) {
        u <- runif(1)
        if (u < p) {
            success <- TRUE
        } else {
            x <- x + 1
        }
    }
    return(x)
}
```

---

## Simulating Continuous Variables

### Cumulative Distribution Function (CDF)

$$F_X(x) = Pr(X \le x)$$

so, $U = F(x) \in (0,1)$.

<embed src="http://upload.wikimedia.org/wikipedia/commons/b/ba/Exponential_cdf.svg" type="image/svg+xml" alt="cdf" />

---

## Inverse Transform Method

We claim that $X = F^{-1}(U)$ is a random variable with CDF *F*.

$$ P(X \le x) = P(F^{-1}(U) \le x) = P(U \le F(x)) = F(x) $$

So if we can generate uniforms and we can calculate quantiles, we can generate non-uniforms!

This is the ***quantile transform method***, or ***inverse transformation method***.

---

## Example: exponential distribution

$$X \sim \text{Exp}(\lambda)$$
$$f_X(x) = \lambda \exp(-\lambda x)$$
$$F_X(x) = 1 - e^{-\lambda x}, \, x \ge 0$$

Solve $F$ for $x$:

$$y = 1 - e^{-\lambda x}$$
$$x = -\frac{1}{\lambda} \log(1-y) = F^{-1}(y)$$

---

## Example: exponential distribution

```{r}
f <- function(x,lam) -1/lam * log(1-x)
hist(f(runif(1000), 3), prob=T, xlab="x", main="")
lines((1:300)/100, dexp((1:300)/100, 3))
```

---

## Rejection Method

We can only use the inverse CDF method if we can calculate $F_X^{-1}$.

$F_X$ can be inverted numerically, using root-finding methods, but this is inefficient.

One alternative approach is the *rejection method*.

Imagine a function with support on some interval $[a,b]$, and for which we can calculate a maximum value, $m = \max(f(x))$.

* simulate points uniformly on $(x, f(x))$:

$$ x \sim U(a,b) $$
$$ f(x) \sim U(0, m) $$

* reject points that fall above the function; remaining points are a sample from $f(x)$.

---

## Rejection Method

![rejection](http://d.pr/i/nmuj+)

---

## Rejection Method

How do we know this simple approach works?

$$ \begin{aligned} Pr(a < X < b) &= Pr(\text{Point sampled under curve}) \\
&= \frac{\text{Area under curve}}{\text{Area of sampling frame}} \\
&= \frac{\int_a^b f(x)\,dx}{1} \\
&= \int_a^b f(x)\,dx
\end{aligned}$$

The *efficiency* of the method depends on the ratio of the function's area to the area of the sampling frame.

---

## Example: Triangular distribution

Consider the triangular pdf:

$$f(x) = \left\{
    \begin{array}{ll}
      x & \text{if } \, 0 < x < 1 \\
      2-x & \text{if } \, 1 \le x < 2 \\
      0 & \text{otherwise}
    \end{array}
\right.
$$

![triangular distribution](http://d.pr/i/3b6g+)

---

## Example: Triangular distribution

The triangular pdf:

```{r}
triangular <- function(x){
    if ((0<x) && (x<1)) {
        return(x)
    } else if ((1<x) && (x<2)) {
        return(2-x)
    } else {
        return(0)
    }
}
```

A function to sample a single realization from `fx` using rejection sampling:

```{r}
rejectionK <- function(fx, a, b, K) {
    while (TRUE) {
        x <- runif(1, a, b)
        y <- runif(1, 0, K)
        if (y < fx(x)) return(x)
    }
}
```

---

## Example: Triangular distribution

![triangular sample](http://d.pr/i/imB7+)

---

## Simulating Normal Variates

With a standard normal random variable, we can transform to an arbitrary normal:

$$N(\mu, \sigma^2) = \mu + \sigma z$$

where $z \sim N(0,1)$.

A simple approach for generating standard normals is to use the central limit theorem to average uniform draws.

For $U \sim \text{Unif}(0,1)$, $E(U) = 1/2$, $\text{Var}(U)=1/12$. Then:

$$Z = \left(\sum_{i=1}^{12} U_i \right) - 6$$

is approximately $N(0,1)$.

## Presenter Notes

Inefficient:: 12 draws to generate one realization

---

## Box-Muller Algorithm

Consider a *bivariate* standard normal variable:

$$ (X,Y) \stackrel{iid}{\sim} N(0,1)$$

Box-Muller simulates $(X,Y)$ in polar coordinates, then transforms them to Cartesian coordinates:

$$X = R \cos(\theta)$$
$$Y = R \sin(\theta)$$

It can be shown that:

$$R^2 \sim \text{Exp}(1/2)$$
$$\theta \sim \text{Unif}(0,2\pi)$$

---

## Box-Muller Algorithm

Algorithm:

1. Generate $U_1, U_2 \sim \text{Unif}(0,1)$
2. Calculate $\theta = 2\pi U_1$, $R = \sqrt{-4\log(U_2)}$
3. Transform $X = R \cos(\theta)$, $Y = R \sin(\theta)$

### Exercise ###

Write a function to generate $n$ variables distributed $N(\mu,\sigma^2)$, using the Box-Muller algorithm.

---

## Bias-corrected Interval

Though they do not assume normality, percentile intervals do not generally perform well.

### Bias-corrected, Accelerated Percentile Intervals

**Step 1**:

Calculate standard normal quantile at the adjusted proportion of bootstrap replicates below the original sample estimate:

$$z = \Phi^{-1}\left[\frac{\sum_{i=1}^R I(T_i^* \le T)}{R+1}\right]$$

* if sampling distribution is symmetric, and $T$ is unbiased, this proportion will be close to 0.5 (z close to zero).

---

## Bias-corrected Interval

**Step 2**:

Let $T_{(-j)^*}$ be the value of $T$ with the *j*th observation held out, and $\bar{T}$ be the average of these holdout estimates. Then,

$$a = \frac{\sum_{j=1}^n(T_{(-j)}^* - \bar{T})^3}{6[\sum_{j=1}^n(T_{(-j)}^* - \bar{T}^2)]^{3/2}}$$

We now have two correction factors, $z$ and $a$.

---

## Bias-corrected Interval

**Step 3**:

Now, calculate adjusted indices to extract endpoints of the interval:

$$a_1 = \Phi\left[z + \frac{z - z_{1-\alpha/2}}{1-a(z - z_{1-\alpha/2})}\right]$$
$$a_2 = \Phi\left[z + \frac{z + z_{1-\alpha/2}}{1-a(z + z_{1-\alpha/2})}\right]$$

The corrected interval is:

$$T_{[R \cdot a_1]}^* < \theta < T_{[R \cdot a_2]}^*$$

## Presenter Notes

- when z and a are both zero, corresponds to uncorrected percentile interval
- R should be on the order of 1000 or more for accuracy

---

## Exercise

Find the coverage probability for bootstrap percentile intervals of the following model:

$$y \sim \text{Gamma}(7,5)$$
